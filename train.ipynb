{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(image_path, label_path):\n",
    "    image = cv.imread(image_path)\n",
    "    image = cv.resize(image, (640, 640))\n",
    "    image /= 255.0\n",
    "\n",
    "    label = np.zeros((80, 80, 18))\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.split()\n",
    "            if line[0] != 0:\n",
    "                continue\n",
    "            x, y, w, h = float(line[1]), float(line[2]), float(line[3]), float(line[4])\n",
    "            if label[int(y * 80)][int(x * 80)][0] == 0:\n",
    "                label[int(y * 80)][int(x * 80)][0:6] = [1, x, y, w, h, 1]\n",
    "            elif label[int(y * 80)][int(x * 80)][6] == 0:\n",
    "                label[int(y * 80)][int(x * 80)][6:12] = [1, x, y, w, h, 1]\n",
    "            elif label[int(y * 80)][int(x * 80)][12] == 0:\n",
    "                label[int(y * 80)][int(x * 80)][12:18] = [1, x, y, w, h, 1]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Custom_Generator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, images, labels, batch_size):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return (np.ceil(len(self.images) / float(self.batch_size))).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.images[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        \n",
    "        train_image = []\n",
    "        train_label = []\n",
    "\n",
    "        for i in range(len(batch_x)):\n",
    "            img_path = batch_x[i]\n",
    "            label = batch_y[i]\n",
    "            image, label = read(img_path, label)\n",
    "            train_image.append(image)\n",
    "            train_label.append(label)\n",
    "        return np.array(batch_x), np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "input_shape = (640,640,3)\n",
    "\n",
    "def identity_block(X, f, filters):\n",
    "    F1, F2 = filters\n",
    "    X_shortcut = X\n",
    "    X = tf.keras.layers.Conv2D(F1, (1,1), strides=(1,1), padding = 'same')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.LeakyReLU(0.1)(X)\n",
    "    X = tf.keras.layers.Conv2D(F2, (f,f), strides=(1,1), padding = 'same')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.LeakyReLU(0.1)(X)\n",
    "    X = tf.keras.layers.Add()([X, X_shortcut])\n",
    "    X = tf.keras.layers.LeakyReLU(0.1)(X)\n",
    "    return X\n",
    "\n",
    "def custom_cnn(input_shape = input_shape, num_classes = num_classes):\n",
    "    X_input = tf.keras.layers.Input(shape=input_shape)\n",
    "    X = tf.keras.layers.Conv2D(32, (3,3), padding = 'same')(X_input)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.LeakyReLU(0.1)(X)\n",
    "    X = tf.keras.layers.Conv2D(64, (3,3), strides=2, padding = 'same')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.LeakyReLU(0.1)(X)\n",
    "    X = identity_block(X, 3, (32,64))\n",
    "    X = tf.keras.layers.Dropout(0.5)(X)\n",
    "    X = tf.keras.layers.Conv2D(128, (3,3), strides=2, padding = 'same')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.LeakyReLU(0.1)(X)\n",
    "    X = identity_block(X, 3, (64,128))\n",
    "    X = identity_block(X, 3, (64,128))\n",
    "    X = tf.keras.layers.Conv2D(256, (3,3), strides=2, padding = 'same')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.LeakyReLU(0.1)(X)\n",
    "    X = identity_block(X, 3, (128,256))\n",
    "    X = identity_block(X, 3, (128,256))\n",
    "    X = identity_block(X, 3, (128,256))\n",
    "    X = identity_block(X, 3, (128,256))\n",
    "    X = identity_block(X, 3, (128,256))\n",
    "    X = identity_block(X, 3, (128,256))\n",
    "    X = identity_block(X, 3, (128,256))\n",
    "    X = identity_block(X, 3, (128,256))\n",
    "    X = tf.keras.layers.Conv2D(128, (1,1), padding = 'same')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.LeakyReLU(0.1)(X)\n",
    "    X = tf.keras.layers.Conv2D(256, (3,3), padding = 'same')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.LeakyReLU(0.1)(X)\n",
    "    X = tf.keras.layers.Conv2D(18, (1,1), padding = 'same')(X)\n",
    "    X = tf.keras.layers.BatchNormalization()(X)\n",
    "    X = tf.keras.layers.LeakyReLU(0.1)(X)\n",
    "    X = tf.keras.models.Model(X_input, X)\n",
    "    return X\n",
    "\n",
    "custom_model = custom_cnn()\n",
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18eea27f2ecd1372e7523d40c6723d85a9133497456635c0a0bd814672b5b852"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
